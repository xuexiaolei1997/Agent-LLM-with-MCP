{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = os.getenv(\"MODEL\")\n",
    "base_url = os.getenv(\"BASE_URL\")\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "model_type = os.getenv(\"MODEL_TYPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(openai_api_base=base_url, model=model, openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = \"你是一个操作系统专家，可以执行终端命令，现在是一台windows操作系统，你可以执行命令并获取系统中的信息。请根据用户的需求执行命令并返回结果。\"\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\", prompt,\n",
    "            ),\n",
    "            (\"placeholder\", \"{messages}\"),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "\n",
    "@tool\n",
    "def exec_cmd(cmd: AnyMessage):\n",
    "    \"\"\"\n",
    "    执行终端命令\n",
    "\n",
    "    Args:\n",
    "        cmd (str): 需要执行的命令\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - output: The standard output of the command.\n",
    "            - error: The standard error of the command.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    import shlex\n",
    "\n",
    "    process = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "    return {\n",
    "        \"output\": output.decode(),\n",
    "        \"error\": error.decode()\n",
    "    }\n",
    "\n",
    "\n",
    "@tool\n",
    "def execute_python_code(code: str) -> str:\n",
    "    \"\"\"执行python代码\n",
    "\n",
    "    Args:\n",
    "        code (str): 需要执行的python代码\n",
    "\n",
    "    Returns:\n",
    "        str: 执行完成后，终端打印的内容\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = subprocess.run(['python', '-c', code], capture_output=True, text=True, timeout=10)\n",
    "        if process.returncode == 0:\n",
    "            return process.stdout.strip()\n",
    "        else:\n",
    "            return f\"Error: {process.stderr.strip()}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Error: Python code execution timed out.\"\n",
    "    except FileNotFoundError:\n",
    "        return \"Error: Python interpreter not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "tools = [exec_cmd, execute_python_code]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools[0].invoke(\"dir D:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    \"\"\"\n",
    "    Function to handle errors that occur during tool execution.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current state of the AI agent, which includes messages and tool call details.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing error messages for each tool that encountered an issue.\n",
    "    \"\"\"\n",
    "    # Retrieve the error from the current state\n",
    "    error = state.get(\"error\")\n",
    "    \n",
    "    # Access the tool calls from the last message in the state's message history\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    \n",
    "    # Return a list of ToolMessages with error details, linked to each tool call ID\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",  # Format the error message for the user\n",
    "                tool_call_id=tc[\"id\"],  # Associate the error message with the corresponding tool call ID\n",
    "            )\n",
    "            for tc in tool_calls  # Iterate over each tool call to produce individual error messages\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    \"\"\"\n",
    "    Function to create a tool node with fallback error handling.\n",
    "    \n",
    "    Args:\n",
    "        tools (list): A list of tools to be included in the node.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A tool node that uses fallback behavior in case of errors.\n",
    "    \"\"\"\n",
    "    # Create a ToolNode with the provided tools and attach a fallback mechanism\n",
    "    # If an error occurs, it will invoke the handle_tool_error function to manage the error\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)],  # Use a lambda function to wrap the error handler\n",
    "        exception_key=\"error\"  # Specify that this fallback is for handling errors\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolnode = create_tool_node_with_fallback(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools[0].args_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tool = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tool.kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_with_tool.invoke(\"我想查看D盘的文件夹，请帮我罗列出来\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.tool_calls[0]['args']['cmd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tools[0].invoke(response.tool_calls[0]['args']['cmd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "class SubGraphState(TypedDict):\n",
    "    message: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "class CustomGraph:\n",
    "    \"\"\"\n",
    "    创建一个图形化的AI代理，使用LLM和工具进行交互。\n",
    "    \"\"\"\n",
    "    def __init__(self, llm, prompt=\"\", tools=[], checkpointer=None):\n",
    "        self.prompt = prompt\n",
    "        self.llm = llm.bind_tools(tools)\n",
    "        self.tools = [{t.name: t} for t in tools]\n",
    "        self.checkpointer = checkpointer\n",
    "        \n",
    "        graph = StateGraph(SubGraphState)\n",
    "        graph.add_node(\"llm\", self.call_llm)\n",
    "        graph.add_node(\"action\", self.call_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        \n",
    "        self.graph = graph.compile(checkpointer=self.checkpointer, interrupt_before=[\"action\"])\n",
    "    \n",
    "    def call_llm(self, state: SubGraphState) -> SubGraphState:\n",
    "        query_message = state[\"message\"]\n",
    "        if self.prompt:\n",
    "            query_message = [SystemMessage(content=self.prompt)] + query_message\n",
    "        message= self.llm.invoke(query_message)\n",
    "        return {\"message\": [message]}\n",
    "\n",
    "    def call_action(self, state: SubGraphState):\n",
    "        # Safely retrieve tool_calls from the last message\n",
    "        last_message = state['message'][-1]\n",
    "        tool_calls = getattr(last_message, 'tool_calls', [])\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if t['name'] in self.tools:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "                results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "            else:\n",
    "                print(f\"Tool {t['name']} not found.\")\n",
    "        print(\"Back to the model!\")\n",
    "        return {'message': results}\n",
    "    \n",
    "    def exists_action(self, state: SubGraphState):\n",
    "        # Safely check if tool_calls exist in the last message\n",
    "        last_message = state['message'][-1]\n",
    "        tool_calls = getattr(last_message, 'tool_calls', [])\n",
    "        return len(tool_calls) > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CustomGraph(llm, prompt, tools, memory)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "message = \"我想查看D盘的文件夹，请帮我罗列出来\"\n",
    "message = HumanMessage(content=message)\n",
    "agent.graph.invoke({\"messages\": message}, thread)  # 这里是一个示例调用，可以根据需要修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    # 初始化一个变量来累积输出\n",
    "    accumulated_output = []\n",
    "\n",
    "    for event in agent.graph.stream({\"messages\": [(\"user\", user_input)]}, thread):\n",
    "        for value in event.values():\n",
    "            # 将模型回复的内容添加到累积输出中\n",
    "            accumulated_output.append(value[\"messages\"][-1].content)\n",
    "\n",
    "    # 返回累积的输出\n",
    "    return accumulated_output\n",
    "\n",
    "# while True:\n",
    "#     try:\n",
    "#         user_input = input(\"用户提问: \")\n",
    "#         if user_input.lower() in [\"退出\", \"quit\"]:\n",
    "#             print(\"下次再见！\")\n",
    "#             break\n",
    "\n",
    "#         # 获取累积的输出\n",
    "#         updates = stream_graph_updates(user_input)\n",
    "\n",
    "#         # 打印最后一个输出\n",
    "#         if updates:\n",
    "#             print(\"模型回复:\")\n",
    "#             print(updates[-1])\n",
    "#     except:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph_updates(\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
